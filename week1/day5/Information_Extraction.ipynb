{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ybyz0S6sQj_T"
   },
   "source": [
    "# Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brzA9z-U_TBc"
   },
   "source": [
    "All assignments were tested in the [Google Colab](https://colab.research.google.com/notebooks/). All core packages are already installed there. \n",
    "\n",
    "If you use your python environment, probably you'll need to install them separately via pip or conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (2.4.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (3.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (3.10.8)\n",
      "Requirement already satisfied: nltk in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (3.9.2)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (1.9.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (1.8.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\renas.alp\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\renas.alp\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\renas.alp\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (from nltk) (4.67.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\renas.alp\\appdata\\roaming\\python\\python314\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\renas.alp\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\renas.alp\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas matplotlib nltk wordcloud scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQjFTtuFFhr1"
   },
   "source": [
    "## Part I. Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBstqhbBEmui"
   },
   "source": [
    "### 1.1 Basic Regex\n",
    "Write a regular expression to search for emails in text.\n",
    "\n",
    "Let's start from the basic emails like john@gmail.com\n",
    "\n",
    "<details>\n",
    "<summary>hints:</summary>\n",
    "\n",
    "you need to catch patterns like word@word.word\n",
    "\n",
    "one of the ways to catch any word character is **\\w** \n",
    "\n",
    "if you need to repeat some character you can use **+**\n",
    "\n",
    "so to catch the full word **\\w+** will be enough\n",
    "\n",
    "if you need to escape some character (because it's a reserved character), you can use **\\\\**, e.g. **\\\\.**\n",
    "\n",
    "you can find all special characters in the [documentation](https://docs.python.org/3/library/re.html)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "E76zlryWCux9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def catch_simple_emails(text: str) -> list:\n",
    "\n",
    "  pattern = r\"\\w+@\\w+\\.\\w+\"\n",
    "  email_regex = re.compile(pattern, re.I)       \n",
    "  emails = email_regex.findall(txt)\n",
    "  return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yrqrn4sEE4Xf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed\n"
     ]
    }
   ],
   "source": [
    "txt = \"Most beginners use generic free business email accounts without a domain name which isn’t very professional. For example: johnsmith2019@gmail.com or jsmithfromstargardening@yahoo.com\"\n",
    "try:\n",
    "  assert catch_simple_emails(txt) == ['johnsmith2019@gmail.com', 'jsmithfromstargardening@yahoo.com']\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(catch_simple_emails(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alm-dBYDK4gM"
   },
   "source": [
    "### 1.2 Complex Regex\n",
    "How can it be modified to catch more complex emails like john.smith@yahoo.co.id or john-smith@yahoo.co.id?\n",
    "\n",
    "Emails also can contain other symbols but they are not required in this assignment\n",
    "<details>\n",
    "<summary>hints:</summary>\n",
    "\n",
    "you need to replace single words with word sequences separated by dots or hyphens\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VvueiNKuFOXF"
   },
   "outputs": [],
   "source": [
    "def catch_complex_emails(txt: str) -> list:\n",
    "  pattern = r\"[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]+\"\n",
    "  email_regex = re.compile(pattern, re.I)       \n",
    "  emails = email_regex.findall(txt)\n",
    "  return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "M6o61KhrGfU_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "simple@example.com nuff said\n",
    "very.common@example.com very.very.common\n",
    "very.common@long.example.com multiple domain levels\n",
    "other.email-with-hyphen@example.com dot + hyphen\n",
    "fully-qualified-domain@example.com hyphen only\n",
    "x@example.com single letter\n",
    "example-indeed@strange-example.com one more example\n",
    "\"\"\"\n",
    "try:\n",
    "  assert catch_complex_emails(txt) == ['simple@example.com',\n",
    "  'very.common@example.com',\n",
    "  'very.common@long.example.com',\n",
    "  'other.email-with-hyphen@example.com',\n",
    "  'fully-qualified-domain@example.com',\n",
    "  'x@example.com',\n",
    "  'example-indeed@strange-example.com']\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(catch_complex_emails(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBsP3w3dWArL"
   },
   "source": [
    "### 1.3 Regex-based Tokenizer\n",
    "\n",
    "Using your knowledge of regular expressions, write a simple text tokenizer that can separate punctuation, numbers, and words into individual tokens.\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "* Floating point numbers like 1.23 fit in one token. The decimal separator can be either a dot or a comma.\n",
    "* The number can be negative and have a sign like -10.5.\n",
    "* There may not be an integer part of the number at all: the sequences -0.15 and -.15 mean the same number.\n",
    "* Empty fractional part is not allowed: string \"10.\" must be split into two tokens: \"10\" and \".\"\n",
    "* Consecutive punctuation marks are separated each into a separate token.\n",
    "* Only English letters are allowed in words.\n",
    "* Empty tokens must be removed\n",
    "\n",
    "<details>\n",
    "<summary>hints:</summary>\n",
    "\n",
    "To combine individual checks into a single regex, you can use the operator | (or), e.g. '[a-z]|[0-9]' means one letter from a to z or one number from 0 to 9\n",
    "\n",
    "Remember that if regex contains multiple blocks, they are processed in natural order (from left to right), so you should catch more complex elements first (e.g., catch floating point numbers before integers); otherwise, you can lose it.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "J5YCRx8bkbq2"
   },
   "outputs": [],
   "source": [
    "def text_split(txt: str) -> list:\n",
    "  pattern = r\"[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]+\"\n",
    "  token_regex = re.compile(pattern, re.I)       \n",
    "  tokens = token_regex.findall(txt)\n",
    "  return tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ymJSyHPblaGE"
   },
   "outputs": [],
   "source": [
    "txt = {\n",
    "  '': [],\n",
    "  '$0.00 \"Surplus\";$-0.00 \"Shortage\"': ['$', '0.00', '\"', 'Surplus', '\"', ';', '$', '-0.00', '\"', 'Shortage', '\"'],\n",
    "  'Combine text and numbers': ['Combine', 'text', 'and', 'numbers'],\n",
    "  'Combine_text_and$$numbers': ['Combine', '_', 'text', '_', 'and', '$', '$', 'numbers'],\n",
    "  '(.4 in this example)': ['(', '.4', 'in', 'this', 'example', ')'],\n",
    "  '(4. in this example)': ['(', '4', '.', 'in', 'this', 'example', ')'],\n",
    "  '$$1,1%#-2': ['$', '$', '1,1', '%', '#', '-2'],\n",
    "  '...': ['.', '.', '.'],\n",
    "  'How to Sauté?': ['How', 'to', 'Saut', 'é', '?']\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JzT6dxu8l9Ty"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed\n",
      "failed\n",
      "[]\n",
      "failed\n",
      "[]\n",
      "failed\n",
      "[]\n",
      "failed\n",
      "[]\n",
      "failed\n",
      "[]\n",
      "failed\n",
      "[]\n",
      "failed\n",
      "[]\n",
      "failed\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for x in txt:\n",
    "  try:\n",
    "    assert text_split(x) == txt[x]\n",
    "    print('passed')\n",
    "  except AssertionError:\n",
    "    print('failed')\n",
    "    print(text_split(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYD-unDcs55m"
   },
   "source": [
    "## Part II. Fuzzy matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkfWi9NRDz13"
   },
   "source": [
    "Fuzzy Matching (also called Approximate String Matching) is a technique that helps identify two elements of text, strings, or entries that are approximately similar but are not exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EISOAKlEtPZ5"
   },
   "source": [
    "### 2.1 Levenshtein distance\n",
    "\n",
    "The Levenshtein distance is a metric used to measure the difference between 2 string sequences.\n",
    "\n",
    "It gives us a measure of the number of single character insertions, deletions or substitutions required to change one string into another.\n",
    "\n",
    " You can find an algorythm and the full description [here](https://en.wikipedia.org/wiki/Levenshtein_distance)\n",
    "\n",
    "For our task we'll use python-levenshtein library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WIPv-N2yGh22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install python-levenshtein # if not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KL4TrXgWtS-i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance between health and Covid is 6\n",
      "distance between chiefs and Covid is 6\n",
      "distance between today and Covid is 4\n",
      "distance between announced and Covid is 7\n",
      "distance between a and Covid is 5\n",
      "distance between fresh and Covid is 5\n",
      "distance between covid and Covid is 1\n",
      "distance between vaccine and Covid is 6\n",
      "distance between programme and Covid is 8\n",
      "distance between ahead and Covid is 4\n",
      "distance between of and Covid is 4\n",
      "distance between what and Covid is 5\n",
      "distance between they and Covid is 5\n",
      "distance between fear and Covid is 5\n",
      "distance between will and Covid is 5\n",
      "distance between be and Covid is 5\n",
      "distance between a and Covid is 5\n",
      "distance between difficult and Covid is 8\n",
      "distance between winter and Covid is 6\n"
     ]
    }
   ],
   "source": [
    "# simple approach\n",
    "import string\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "txt = 'Health chiefs today announced a fresh Covid vaccine programme ahead of what they fear will be a difficult winter.'\n",
    "clean_txt = ''.join([c for c in txt.lower() if c not in string.punctuation])\n",
    "words = [i for i in clean_txt.split()]\n",
    "pattern = 'Covid'\n",
    "\n",
    "for word in words:\n",
    "  print(f\"distance between {word} and {pattern} is {levenshtein_distance(word, pattern)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24vG07zHIhR_"
   },
   "source": [
    "Your task.\n",
    "Implement a function that will return the word from the text that is most similar to the given pattern.\n",
    "\n",
    "If there are many words with the same distance, please use the first one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CE6Kf4GyGpg8"
   },
   "outputs": [],
   "source": [
    "def find_similar_word(txt: str, pattern: str) -> str:\n",
    "\n",
    "  out = ''\n",
    "  # PUT YOUR CODE HERE\n",
    "  import string\n",
    "  from Levenshtein import distance as levenshtein_distance\n",
    "  clean_txt = ''.join([c for c in txt.lower() if c not in string.punctuation])\n",
    "  words = clean_txt.split()\n",
    "  min_dist = float('inf')\n",
    "  for word in words:\n",
    "      dist = levenshtein_distance(word, pattern)\n",
    "      if dist < min_dist:\n",
    "          min_dist = dist\n",
    "          out = word\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "US6hul6lGpkG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed\n"
     ]
    }
   ],
   "source": [
    "txt = 'spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.'\n",
    "try:\n",
    "  assert find_similar_word(txt, 'dependent') == 'dependency'\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(find_similar_word(txt, 'dependent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w5sIfmFTxvU"
   },
   "source": [
    "Implement a function that will return the pair of most similar words in the given text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7J3ibh4uUjAA"
   },
   "outputs": [],
   "source": [
    "def find_similar_words(txt: str) -> list: # output should be like ['word1', 'word2']\n",
    "    out = []\n",
    "    clean_txt = ''.join([c for c in txt.lower() if c not in string.punctuation])\n",
    "    words = clean_txt.split()\n",
    "    min_dist = float('inf')\n",
    "    pair = ('', '')\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i+1, len(words)):\n",
    "            dist = levenshtein_distance(words[i], words[j])\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                pair = (words[i], words[j])\n",
    "    out = list(pair)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5X0P-UqiVia1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed\n"
     ]
    }
   ],
   "source": [
    "txt = 'Sentence splitting is the process of dividing text into sentences'\n",
    "try:\n",
    "  assert find_similar_words(txt) == ['sentence', 'sentences']\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(find_similar_words(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An8ReNtZbU9B"
   },
   "source": [
    "You can find an additional fuzzy matching component for SpaCy here:\n",
    "https://github.com/gandersen101/spaczz\n",
    "\n",
    "There are many different libraries for this, such as tfidf-matcher, fuzzywuzzy, rapidfuzz, etc.\n",
    "\n",
    "For simplicity, we will not touch them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A8htU4Vyrv4"
   },
   "source": [
    "## Part III. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArQl_x1lzbJp"
   },
   "source": [
    "Named-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\n",
    "For this task it is better to use either **NLTK** or **SpaCy** libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOe2NSso-_2e"
   },
   "source": [
    "### 3.1 NLTK Approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qGGNZmfb0OF7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\renas.alp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cfdsfLWy0Rlt"
   },
   "outputs": [],
   "source": [
    "def nltk_ner(txt: str) -> list:\n",
    "\n",
    "  out = []\n",
    "\n",
    "  for sent in nltk.sent_tokenize(txt):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "      if hasattr(chunk, 'label'):\n",
    "          out.append(f\"{chunk.label()} {' '.join(c[0] for c in chunk)}\")\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "P48ljJYl3Lw0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PERSON Boris', 'PERSON Johnson', 'ORGANIZATION Chequers']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"Boris Johnson skipped out on his second emergency meeting in a row today and tomorrow he is hosting a lavish goodbye bash at Chequers. A spokesman said Sunday's party was a 'private event'.\"\n",
    "nltk_ner(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "iTFdEgo8WQbv"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,1368.0,168.0\" width=\"1368px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"4.67836%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Boris</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"2.33918%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.26316%\" x=\"4.67836%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Johnson</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.30994%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.26316%\" x=\"9.94152%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">skipped</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.5731%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.92398%\" x=\"15.2047%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">out</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.33918%\" x=\"18.1287%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.2982%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.50877%\" x=\"20.4678%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">his</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.2222%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.67836%\" x=\"23.9766%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">second</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.3158%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.43275%\" x=\"28.655%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">emergency</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.8713%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.26316%\" x=\"35.0877%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">meeting</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.7193%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.33918%\" x=\"40.3509%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.5205%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.33918%\" x=\"42.6901%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"43.8596%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.92398%\" x=\"45.0292%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">row</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.4912%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09357%\" x=\"47.9532%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">today</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.92398%\" x=\"52.0468%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.5088%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.84795%\" x=\"54.9708%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">tomorrow</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.8947%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.92398%\" x=\"60.8187%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">he</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.2807%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.92398%\" x=\"63.7427%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.2047%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.26316%\" x=\"66.6667%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">hosting</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.2982%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.33918%\" x=\"71.9298%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.0994%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.67836%\" x=\"74.269%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">lavish</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6082%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.26316%\" x=\"78.9474%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">goodbye</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.5789%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.50877%\" x=\"84.2105%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">bash</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.9649%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.33918%\" x=\"87.7193%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.8889%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.18713%\" x=\"90.0585%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Chequers</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.152%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.75439%\" x=\"98.2456%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.1228%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Boris', 'NNP')]), Tree('PERSON', [('Johnson', 'NNP')]), ('skipped', 'VBD'), ('out', 'RP'), ('on', 'IN'), ('his', 'PRP$'), ('second', 'JJ'), ('emergency', 'NN'), ('meeting', 'NN'), ('in', 'IN'), ('a', 'DT'), ('row', 'NN'), ('today', 'NN'), ('and', 'CC'), ('tomorrow', 'NN'), ('he', 'PRP'), ('is', 'VBZ'), ('hosting', 'VBG'), ('a', 'DT'), ('lavish', 'JJ'), ('goodbye', 'NN'), ('bash', 'NN'), ('at', 'IN'), Tree('ORGANIZATION', [('Chequers', 'NNP')]), ('.', '.')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(nltk.sent_tokenize(txt)[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k80aW_vC2lKG"
   },
   "source": [
    "### 3.2 SpaCy Approach\n",
    "Your task.\n",
    "\n",
    "Repeat the same procedure using the Spacy library. Compare the results of both functions. What can you say about it?\n",
    "\n",
    "list of all useful properties:  [Doc](https://spacy.io/api/doc#ents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "k7ZuIX2b0OsT"
   },
   "outputs": [],
   "source": [
    "r = r\"C:\\Python3XX\\Scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0jZNLxNQ4Juz"
   },
   "outputs": [],
   "source": [
    "def spacy_ner(txt: str) -> list: # output should be like ['label1 word1', 'label2 word2', ...]\n",
    "    out = []\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(txt)\n",
    "    for ent in doc.ents:\n",
    "        out.append(f\"{ent.label_} {ent.text}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Sib8gDjk0skw"
   },
   "outputs": [
    {
     "ename": "ConfigError",
     "evalue": "unable to infer type for attribute \"REGEX\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConfigError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m txt = \u001b[33m\"\u001b[39m\u001b[33mBoris Johnson skipped out on his second emergency meeting in a row today and tomorrow he is hosting a lavish goodbye bash at Chequers. A spokesman said Sunday\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms party was a \u001b[39m\u001b[33m'\u001b[39m\u001b[33mprivate event\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mspacy_ner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m == [\u001b[33m'\u001b[39m\u001b[33mPERSON Boris Johnson\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mORDINAL second\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDATE today\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDATE tomorrow\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDATE Sunday\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mpassed\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mspacy_ner\u001b[39m\u001b[34m(txt)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mspacy_ner\u001b[39m(txt: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m: \u001b[38;5;66;03m# output should be like ['label1 word1', 'label2 word2', ...]\u001b[39;00m\n\u001b[32m      2\u001b[39m     out = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m      4\u001b[39m     nlp = spacy.load(\u001b[33m'\u001b[39m\u001b[33men_core_web_sm\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m     doc = nlp(txt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\__init__.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthinc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\pipeline\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mattributeruler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdep_parser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01medit_tree_lemmatizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\pipeline\\attributeruler.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmatcher\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscorer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\language.py:46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlang\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenizer_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BASE_EXCEPTIONS, URL_MATCH\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlookups\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_lookups\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipe_analysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschemas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     48\u001b[39m     ConfigSchema,\n\u001b[32m     49\u001b[39m     ConfigSchemaInit,\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     validate_init_settings,\n\u001b[32m     53\u001b[39m )\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscorer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\pipe_analysis.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwasabi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokens\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Doc, Span, Token\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dot_to_dict\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\tokens\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_serialize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmorphanalysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MorphAnalysis\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\tokens\\_serialize.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleFrozenList, ensure_path\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvocab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dict_proxies\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpanGroups\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DOCBIN_ALL_ATTRS \u001b[38;5;28;01mas\u001b[39;00m ALL_ATTRS\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\vocab.pyx:1\u001b[39m, in \u001b[36minit spacy.vocab\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\tokens\\doc.pyx:49\u001b[39m, in \u001b[36minit spacy.tokens.doc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\schemas.py:195\u001b[39m\n\u001b[32m    191\u001b[39m         obj = converted\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m validate(TokenPatternSchema, {\u001b[33m\"\u001b[39m\u001b[33mpattern\u001b[39m\u001b[33m\"\u001b[39m: obj})\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mTokenPatternString\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mREGEX\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mStrictStr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTokenPatternString\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mregex\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mIN\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mStrictStr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43min\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pydantic\\v1\\main.py:221\u001b[39m, in \u001b[36mModelMetaclass.__new__\u001b[39m\u001b[34m(mcs, name, bases, namespace, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_valid_field(var_name) \u001b[38;5;129;01mand\u001b[39;00m var_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m annotations \u001b[38;5;129;01mand\u001b[39;00m can_be_changed:\n\u001b[32m    220\u001b[39m     validate_field_name(bases, var_name)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     inferred = \u001b[43mModelField\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUndefined\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_validators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(inferred.type_, fields[var_name].type_):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pydantic\\v1\\fields.py:504\u001b[39m, in \u001b[36mModelField.infer\u001b[39m\u001b[34m(cls, name, value, annotation, class_validators, config)\u001b[39m\n\u001b[32m    501\u001b[39m     required = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    502\u001b[39m annotation = get_annotation_from_field_info(annotation, field_info, name, config.validate_assignment)\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43malias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequired\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequired\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pydantic\\v1\\fields.py:434\u001b[39m, in \u001b[36mModelField.__init__\u001b[39m\u001b[34m(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28mself\u001b[39m.shape: \u001b[38;5;28mint\u001b[39m = SHAPE_SINGLETON\n\u001b[32m    433\u001b[39m \u001b[38;5;28mself\u001b[39m.model_config.prepare_field(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pydantic\\v1\\fields.py:544\u001b[39m, in \u001b[36mModelField.prepare\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    538\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[33;03m    Prepare the field but inspecting self.default, self.type_ etc.\u001b[39;00m\n\u001b[32m    540\u001b[39m \n\u001b[32m    541\u001b[39m \u001b[33;03m    Note: this method is **not** idempotent (because _type_analysis is not idempotent),\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03m    e.g. calling it it multiple times may modify the field and configure it incorrectly.\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_default_and_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m ForwardRef \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m DeferredType:\n\u001b[32m    546\u001b[39m         \u001b[38;5;66;03m# self.type_ is currently a ForwardRef and there's nothing we can do now,\u001b[39;00m\n\u001b[32m    547\u001b[39m         \u001b[38;5;66;03m# user will need to call model.update_forward_refs()\u001b[39;00m\n\u001b[32m    548\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pydantic\\v1\\fields.py:576\u001b[39m, in \u001b[36mModelField._set_default_and_type\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    573\u001b[39m     \u001b[38;5;28mself\u001b[39m.annotation = \u001b[38;5;28mself\u001b[39m.type_\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_ \u001b[38;5;129;01mis\u001b[39;00m Undefined:\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m errors_.ConfigError(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33munable to infer type for attribute \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.required \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28mself\u001b[39m.allow_none = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConfigError\u001b[39m: unable to infer type for attribute \"REGEX\""
     ]
    }
   ],
   "source": [
    "txt = \"Boris Johnson skipped out on his second emergency meeting in a row today and tomorrow he is hosting a lavish goodbye bash at Chequers. A spokesman said Sunday's party was a 'private event'.\"\n",
    "try:\n",
    "  assert spacy_ner(txt) == ['PERSON Boris Johnson', 'ORDINAL second', 'DATE today', 'DATE tomorrow', 'DATE Sunday']\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(spacy_ner(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZpX0e4U5ucI"
   },
   "source": [
    "SpaCy can also render named entities so you can check yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gCUQQzcB7R7K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renas.alp\\AppData\\Roaming\\Python\\Python314\\site-packages\\confection\\__init__.py:38: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1 import BaseModel, Extra, ValidationError, create_model\n"
     ]
    },
    {
     "ename": "ConfigError",
     "evalue": "unable to infer type for attribute \"REGEX\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConfigError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m displacy  \n\u001b[32m      2\u001b[39m displacy.render(nlp(txt), jupyter=\u001b[38;5;28;01mTrue\u001b[39;00m, style=\u001b[33m'\u001b[39m\u001b[33ment\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\__init__.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthinc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\pipeline\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mattributeruler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdep_parser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01medit_tree_lemmatizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\pipeline\\attributeruler.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmatcher\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscorer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\language.py:46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlang\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenizer_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BASE_EXCEPTIONS, URL_MATCH\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlookups\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_lookups\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipe_analysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschemas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     48\u001b[39m     ConfigSchema,\n\u001b[32m     49\u001b[39m     ConfigSchemaInit,\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     validate_init_settings,\n\u001b[32m     53\u001b[39m )\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscorer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\pipe_analysis.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwasabi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokens\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Doc, Span, Token\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dot_to_dict\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\tokens\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_serialize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmorphanalysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MorphAnalysis\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\tokens\\_serialize.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleFrozenList, ensure_path\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvocab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dict_proxies\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpanGroups\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DOCBIN_ALL_ATTRS \u001b[38;5;28;01mas\u001b[39;00m ALL_ATTRS\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\vocab.pyx:1\u001b[39m, in \u001b[36minit spacy.vocab\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\tokens\\doc.pyx:49\u001b[39m, in \u001b[36minit spacy.tokens.doc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\spacy\\schemas.py:195\u001b[39m\n\u001b[32m    191\u001b[39m         obj = converted\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m validate(TokenPatternSchema, {\u001b[33m\"\u001b[39m\u001b[33mpattern\u001b[39m\u001b[33m\"\u001b[39m: obj})\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mTokenPatternString\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mREGEX\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mStrictStr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTokenPatternString\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mregex\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mIN\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mStrictStr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43min\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pydantic\\v1\\main.py:221\u001b[39m, in \u001b[36mModelMetaclass.__new__\u001b[39m\u001b[34m(mcs, name, bases, namespace, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_valid_field(var_name) \u001b[38;5;129;01mand\u001b[39;00m var_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m annotations \u001b[38;5;129;01mand\u001b[39;00m can_be_changed:\n\u001b[32m    220\u001b[39m     validate_field_name(bases, var_name)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     inferred = \u001b[43mModelField\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUndefined\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_validators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(inferred.type_, fields[var_name].type_):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pydantic\\v1\\fields.py:504\u001b[39m, in \u001b[36mModelField.infer\u001b[39m\u001b[34m(cls, name, value, annotation, class_validators, config)\u001b[39m\n\u001b[32m    501\u001b[39m     required = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    502\u001b[39m annotation = get_annotation_from_field_info(annotation, field_info, name, config.validate_assignment)\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43malias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequired\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequired\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pydantic\\v1\\fields.py:434\u001b[39m, in \u001b[36mModelField.__init__\u001b[39m\u001b[34m(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28mself\u001b[39m.shape: \u001b[38;5;28mint\u001b[39m = SHAPE_SINGLETON\n\u001b[32m    433\u001b[39m \u001b[38;5;28mself\u001b[39m.model_config.prepare_field(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pydantic\\v1\\fields.py:544\u001b[39m, in \u001b[36mModelField.prepare\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    538\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[33;03m    Prepare the field but inspecting self.default, self.type_ etc.\u001b[39;00m\n\u001b[32m    540\u001b[39m \n\u001b[32m    541\u001b[39m \u001b[33;03m    Note: this method is **not** idempotent (because _type_analysis is not idempotent),\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03m    e.g. calling it it multiple times may modify the field and configure it incorrectly.\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_default_and_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m ForwardRef \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m DeferredType:\n\u001b[32m    546\u001b[39m         \u001b[38;5;66;03m# self.type_ is currently a ForwardRef and there's nothing we can do now,\u001b[39;00m\n\u001b[32m    547\u001b[39m         \u001b[38;5;66;03m# user will need to call model.update_forward_refs()\u001b[39;00m\n\u001b[32m    548\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pydantic\\v1\\fields.py:576\u001b[39m, in \u001b[36mModelField._set_default_and_type\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    573\u001b[39m     \u001b[38;5;28mself\u001b[39m.annotation = \u001b[38;5;28mself\u001b[39m.type_\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_ \u001b[38;5;129;01mis\u001b[39;00m Undefined:\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m errors_.ConfigError(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33munable to infer type for attribute \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.required \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28mself\u001b[39m.allow_none = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConfigError\u001b[39m: unable to infer type for attribute \"REGEX\""
     ]
    }
   ],
   "source": [
    "from spacy import displacy  \n",
    "displacy.render(nlp(txt), jupyter=True, style='ent')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eBstqhbBEmui",
    "alm-dBYDK4gM",
    "WBsP3w3dWArL",
    "EISOAKlEtPZ5",
    "iOe2NSso-_2e",
    "k80aW_vC2lKG"
   ],
   "name": "Information Extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
